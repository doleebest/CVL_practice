{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to apply to Video! </br>\n",
    "the next step would be to analyze these for the whole video. </br>\n",
    "So keep the background image, load every new frame of the video, run your calculations, and then save it to a video file. This tutorial might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error opening the video file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"Resources/Cars.mp4\"\n"
     ]
    }
   ],
   "source": [
    "# example codes\n",
    "# reference : https://learnopencv.com/reading-and-writing-videos-using-opencv/\n",
    "import cv2 \n",
    " \n",
    "# Create a video capture object, in this case we are reading the video from a file\n",
    "vid_capture = cv2.VideoCapture('Resources/Cars.mp4')\n",
    " \n",
    "if (vid_capture.isOpened() == False):\n",
    "  print(\"Error opening the video file\")\n",
    "# Read fps and frame count\n",
    "else:\n",
    "  # Get frame rate information\n",
    "  # You can replace 5 with CAP_PROP_FPS as well, they are enumerations\n",
    "  fps = vid_capture.get(5)\n",
    "  print('Frames per second : ', fps,'FPS')\n",
    " \n",
    "  # Get frame count\n",
    "  # You can replace 7 with CAP_PROP_FRAME_COUNT as well, they are enumerations\n",
    "  frame_count = vid_capture.get(7)\n",
    "  print('Frame count : ', frame_count)\n",
    " \n",
    "while(vid_capture.isOpened()):\n",
    "  # vid_capture.read() methods returns a tuple, first element is a bool \n",
    "  # and the second is frame\n",
    "  ret, frame = vid_capture.read()\n",
    "  if ret == True:\n",
    "    cv2.imshow('Frame',frame)\n",
    "    # 20 is in milliseconds, try to increase the value, say 50 and observe\n",
    "    key = cv2.waitKey(20)\n",
    "     \n",
    "    if key == ord('q'):\n",
    "      break\n",
    "  else:\n",
    "    break\n",
    " \n",
    "# Release the video capture object\n",
    "vid_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m depth_image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(depth_directory, filename)) \u001b[38;5;66;03m# 파일을 읽어옴\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# take depth difference\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m diff \u001b[38;5;241m=\u001b[39m depth_image\u001b[38;5;241m-\u001b[39mbackground_depth\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m## process bounding box - copied from 'practice1.ipynb'\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Normalize the difference image to range [0, 255]\u001b[39;00m\n\u001b[1;32m     39\u001b[0m normalized_diff \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mnormalize(np\u001b[38;5;241m.\u001b[39mabs(diff), \u001b[38;5;28;01mNone\u001b[39;00m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m255\u001b[39m, norm_type\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mNORM_MINMAX, dtype\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mCV_8U)\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "color_directory = \"2018-04-20-10-04-11/kinect/color\"\n",
    "depth_directory = \"2018-04-20-10-04-11/kinect/depth\"\n",
    "\n",
    "color_filenames = os.listdir(color_directory)\n",
    "depth_filenames = os.listdir(depth_directory)\n",
    "\n",
    "background_depth = cv2.imread(os.path.join(depth_directory, depth_filenames[0]))\n",
    "\n",
    "output_filename = 'output_video.avi'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID') # 비디오 코덱을 지정하는 4자 코드\n",
    "fps = 25.0 # 초당 프레임 수\n",
    "frame_size = (640, 480) # 프레임 크기 (예: 가로 640, 세로 480)\n",
    "result_vid = cv2.VideoWriter(output_filename, fourcc, fps, frame_size) # VideoWriter 객체 생성\n",
    "# with color_filenames, convert to frames and store as video \n",
    "for filename in color_filenames:\n",
    "    frame = cv2.imread(os.path.join(color_directory, filename))  # 파일을 읽어 프레임으로 변환\n",
    "    result_vid.write(frame)  # 프레임을 결과 비디오에 추가\n",
    "\n",
    "\n",
    "#result_vid.write(color_filenames)\n",
    "result_vid.release() \n",
    "\n",
    "prev_bounding_box = ...\n",
    "\n",
    "for filename in depth_filenames:\n",
    "\n",
    "    # load Image\n",
    "    depth_image = cv2.imread(os.path.join(depth_directory, filename)) # 파일을 읽어옴\n",
    "    # take depth difference\n",
    "    diff = depth_image-background_depth\n",
    "\n",
    "    ## process bounding box - copied from 'practice1.ipynb'\n",
    "    # Normalize the difference image to range [0, 255]\n",
    "    normalized_diff = cv2.normalize(np.abs(diff), None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    # Binarize\n",
    "    _, im_th = cv2.threshold(normalized_diff, 30, 255, cv2.THRESH_BINARY)\n",
    "    # Taking a matrix of size 5 as the kernel ; gernerate 5*5 array filled with 1\n",
    "    kernel = np.ones((5, 5), np.uint8) \n",
    "\n",
    "    img_erosion = cv2.erode(im_th, kernel, iterations=3) \n",
    "    img_dilation = cv2.dilate(im_th, kernel, iterations=3) \n",
    "    # Get all group of pixels using cv.connectedComponentsWithStats()\n",
    "    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(img_erosion)\n",
    "\n",
    "    # Step 4: Filter out small areas and draw bounding boxes\n",
    "    threshold = 100\n",
    "    # Iterate through stats to filter out small areas and draw bounding boxes\n",
    "    for i in range(1, num_labels): # num_labels : connected number of labels\n",
    "        area = stats[i, cv2.CC_STAT_AREA] # stats[i, cv2.CC_STAT_AREA]is the number of pixels(area) of ith label(group)\n",
    "        if area > threshold:\n",
    "            x, y, w, h = stats[i, cv2.CC_STAT_LEFT], stats[i, cv2.CC_STAT_TOP], stats[i, cv2.CC_STAT_WIDTH], stats[i, cv2.CC_STAT_HEIGHT]\n",
    "            cv2.rectangle(color_filenames[i], (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw bounding box on color image\n",
    "\n",
    "    # Display the result\n",
    "    plt.imshow(cv2.cvtColor(depth_image, cv2.COLOR_BGR2RGB)) # Convert BGR to RGB for proper display with matplotlib\n",
    "    plt.show()\n",
    "\n",
    "    result_vid.write(depth_image)\n",
    "\n",
    "result_vid.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the bounding box i/u ; later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ai -> color image\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# camera capture color image only\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# depth image only\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# torchvision models - models and weights ~~ 사이트 들어가기 -> object detection model!! 사용하면 됨\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# yolo pytorch hub : codes 있음\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124multralytics/yolov5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myolov5s\u001b[39m\u001b[38;5;124m'\u001b[39m, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# ai -> color image\n",
    "# camera capture color image only\n",
    "# depth image only\n",
    "\n",
    "# ai -> bounding box on color image\n",
    "# train -> depth image  only for\n",
    "# fine tuning\n",
    "\n",
    "# pytorch model zoo\n",
    "# torchvision models - models and weights ~~ 사이트 들어가기 -> object detection model!! 사용하면 됨\n",
    "# yolo pytorch hub : codes 있음\n",
    "import torch\n",
    "\n",
    "# Model\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Images\n",
    "imgs = ['https://ultralytics.com/images/zidane.jpg']  # batch of images\n",
    "\n",
    "# Inference\n",
    "results = model(imgs)\n",
    "\n",
    "# Results\n",
    "results.print()\n",
    "results.show()  # or .show()\n",
    "\n",
    "results.xyxy[0]  # img1 predictions (tensor)\n",
    "results.pandas().xyxy[0]  # img1 predictions (pandas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
